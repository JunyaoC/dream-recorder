sequenceDiagram
    autonumber
    
    box Hardware Components
        participant TS as Touch Sensor (TTP223B)
        participant MIC as USB Microphone
        participant RPI as Raspberry Pi 5
        participant SCRN as Display Screen
    end
    
    box Software Components
        participant SYSTEMD as Systemd Services
        participant CHROME as Chrome Kiosk
        participant FE as Frontend (Web Interface)
        participant WS as WebSocket Server
        participant BE as Backend (Flask)
        participant GPIO_SVC as GPIO Service
        participant GP as GPIO Controller
        participant FFMPEG as FFmpeg Processor
    end
    
    box External Services
        participant OpenAI as OpenAI API
        participant LL as LumaLabs API
    end
    
    Note over SYSTEMD,CHROME: Step 0: System Startup
    SYSTEMD ->> BE: Start Dream Recorder Service
    BE ->> GPIO_SVC: Initialize GPIO Service
    SYSTEMD ->> CHROME: Start Kiosk Mode (after main service)
    CHROME ->> SCRN: Display Web Interface in Fullscreen
    
    Note over TS,SCRN: Step 1: Initial Setup & Recording
    TS ->> GP: User single taps sensor (wake device)
    GP ->> GPIO_SVC: Detects single tap pattern
    GPIO_SVC ->> BE: Sends wake device request
    BE ->> FE: Notifies to wake up UI
    
    TS ->> GP: User long presses sensor (start recording)
    GP ->> GPIO_SVC: Detects long press pattern
    GPIO_SVC ->> BE: Sends recording start request
    BE ->> FE: Notifies to start recording
    FE ->> FE: Updates UI (recording state)
    MIC ->> FE: Captures audio stream
    FE ->> WS: Streams audio data via WebSockets
    
    Note over WS,OpenAI: Step 2: Speech Processing
    TS ->> GP: User releases long press (stop recording)
    GP ->> GPIO_SVC: Detects long press release
    GPIO_SVC ->> BE: Sends recording stop request
    BE ->> FE: Notifies to stop recording
    FE ->> FE: Updates UI (processing state)
    WS ->> OpenAI: Forwards complete audio data for STT
    OpenAI -->> WS: Returns transcribed text
    
    Note over BE,OpenAI: Step 3: Prompt Generation
    BE ->> OpenAI: Sends transcribed text to create video prompt
    OpenAI -->> BE: Returns enhanced video prompt
    BE ->> FE: Updates UI with transcription
    
    Note over BE,LL: Step 4: Video Generation
    BE ->> LL: Sends video generation prompt to LumaLabs
    BE ->> FE: Updates UI (generating video state)
    
    Note over BE,FE: Step 5: Waiting for Video
    BE -->> BE: Polling LumaLabs API for video status
    LL -->> BE: Returns video URL when complete
    
    Note over BE,FFMPEG: Step 6: Video Post-Processing
    BE ->> BE: Downloads video from URL
    BE ->> FFMPEG: Sends video for post-processing
    FFMPEG -->> BE: Returns processed video file
    BE ->> FE: Updates UI (processing video)
    
    Note over BE,SCRN: Step 7: Video Playback
    BE ->> FE: Sends processed video location
    FE ->> FE: Loads processed video
    FE ->> SCRN: Displays video on screen
    
    Note over TS,SCRN: Step 8: User Interactions
    TS ->> GP: User double taps sensor
    GP ->> GPIO_SVC: Detects double tap pattern
    GPIO_SVC ->> BE: Sends show previous dream request
    BE ->> FE: Notifies to load previous dream
    FE ->> SCRN: Displays previous dream video
    
    Note over RPI,SCRN: System Ready for Next Dream
    BE ->> FE: Resets UI to ready state 